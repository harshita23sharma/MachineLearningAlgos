{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwPef832j5Ae"
      },
      "outputs": [],
      "source": [
        "# kaggle competitions download -c jigsaw-toxic-comment-classification-challenge\n",
        "competition_name = \"jigsaw-toxic-comment-classification-challenge\"\n",
        "\n",
        "# Mount your Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#text cleaning\n",
        "import regex as re\n",
        "import string"
      ],
      "metadata": {
        "id": "mUlp_SBDnLOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/projects/interpretability/kaggle_data/train.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "zmJReql4nPQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "NS9NM5E8nZXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"clean_text\"] = df.comment_text.apply(lambda x : clean_text(x))"
      ],
      "metadata": {
        "id": "MZi8eDJGnsGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.clean_text\n",
        "y = df[classes].values"
      ],
      "metadata": {
        "id": "KrFvf19mnhro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Tokenizer\n"
      ],
      "metadata": {
        "id": "F8okA0htnxVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer"
      ],
      "metadata": {
        "id": "G67p71uFnwSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 32000 #(hyper param)"
      ],
      "metadata": {
        "id": "0erj41AfagBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/drive/MyDrive/projects/tokenizer/"
      ],
      "metadata": {
        "id": "FdaOutyyt2dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_path = '/content/drive/MyDrive/your_path'\n",
        "\n",
        "\n",
        "special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"]\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "tokenizer.train_from_iterator(\n",
        "    iterator=X, vocab_size=vocab_size, min_frequency=2, show_progress=True, special_tokens=special_tokens\n",
        ")\n",
        "tokenizer.save_model(tokenizer_path)"
      ],
      "metadata": {
        "id": "hETXoEOsnnfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Will take 1710 as max_seq_len"
      ],
      "metadata": {
        "id": "ddN5H1a0RTO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[\"encoded_ids\"] = X.apply(lambda x: len(tokenizer_loaded.encode(x)) if type(x)== str else 0)\n",
        "max(X[\"encoded_ids\"].values) #1709"
      ],
      "metadata": {
        "id": "HtFgM734O5fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "import os\n",
        "\n",
        "\n",
        "def load_tokenizer(load_path):\n",
        "      # Load tokenizer from the temporary path\n",
        "      tokenizer = ByteLevelBPETokenizer(load_path+ '/vocab.json', load_path + '/merges.txt')\n",
        "      return tokenizer"
      ],
      "metadata": {
        "id": "a1fx0THXnnz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_path = '/content/drive/MyDrive/projects/tokenizer/'\n",
        "tokenizer_loaded = load_tokenizer(tokenizer_path)"
      ],
      "metadata": {
        "id": "uZyHLP9noJKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, tokenizer, hparams):\n",
        "  encoding = tokenizer.encode(text)\n",
        "  encoding.truncate(max_length=hparams.max_seq_len)\n",
        "  encoding.pad(length=hparams.max_seq_len)\n",
        "  return encoding.ids\n"
      ],
      "metadata": {
        "id": "V1zsPqumoKz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = encode(X[0],tokenizer_loaded, hparams)"
      ],
      "metadata": {
        "id": "44p_HpFjoY2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sample(sample, tokenizer):\n",
        "  input_ids = encode(sample,tokenizer,hparams)\n",
        "  return input_ids"
      ],
      "metadata": {
        "id": "zrlw-FUgo7RS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}