{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression using gradient Descent\n",
    "\n",
    "- Y = Xi_0 * w_0 + Xi_1 * w_1 +... + Xi_n * w_n + b\n",
    "\n",
    "- The logistic regression function predicts the probability that an observation is class 1, given the x features. It can be rearranged into the format of the sigmoid function.\n",
    "\n",
    "- sigmoid(Y) = 1 / (1 + e^-Y)\n",
    "\n",
    "- Loss function = Y*log(P(X)) \n",
    "- So, we need to calculate the gradient of the sigmoid function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumptions of Logistic Regression\n",
    "- Binary logistic regression requires the dependent variable to be binary.\n",
    "- For a binary regression, the factor level 1 of the dependent variable should represent the desired outcome.\n",
    "- Only the meaningful variables should be included.\n",
    "- The independent variables should be independent of each other. That is, the model should have little or no multicollinearity.\n",
    "- The independent variables are linearly related to the log odds.\n",
    "- Logistic regression requires quite large sample sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have the prediction, you can apply the basic gradient descent algorithm to optimize your model parameters, which are the weights and bias in this case. You do not use stochastic (or mini-batch gradient descent) in this notebook\n",
    "\n",
    "- w_t+1 = w_t - lr * gradient of loss with respect to y, \n",
    "- where loss is the cross-entropy loss\n",
    "\n",
    "- L(p,y) = (-1/m)(Sigma(ylog(p)+ (1-y)log(1-p)))\n",
    "\n",
    "- gradient of loss = 1/m(p-y)  This can be used to update the weights & biases in gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) calculate w*x + b\n",
    "2) Take sigmoid and get pred = sigmoid(w*x + b)\n",
    "3) loss = (-1/m)(Sigma(ylog(p)+ (1-y)log(1-p)))\n",
    "4) Calculate gradient and get error_w, error_b = compute_gradients(x, y, pred)\n",
    "5) update parameters(error_w, error_b)\n",
    "6) Again predict\n",
    "7) Calculate accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.413564</td>\n",
       "      <td>0.730614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.770186</td>\n",
       "      <td>1.430121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.253314</td>\n",
       "      <td>1.363463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels        X1        X2\n",
       "0       0 -0.413564  0.730614\n",
       "1       0 -0.770186  1.430121\n",
       "2       0 -0.253314  1.363463"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#some simulated data\n",
    "Y = np.sort(np.random.randint(low=0, high=2, size=1000))\n",
    "X1_0 = np.random.normal(loc=0, scale=1, size=500)\n",
    "X1_1 = np.random.normal(loc=5, scale=1, size=500)\n",
    "X1 = np.hstack((X1_0, X1_1))\n",
    "X2_0 = np.random.normal(loc=0, scale=1, size=500)\n",
    "X2_1 = np.random.normal(loc=2, scale=1, size=500)\n",
    "X2 = np.hstack((X2_0, X2_1))\n",
    "X = np.vstack((X1, X2))\n",
    "\n",
    "df = pd.DataFrame([Y, X1, X2]).T\n",
    "df.columns = [\"labels\", \"X1\", \"X2\"]\n",
    "df[\"labels\"] = df.labels.astype('int')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionFromScratch():\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.train_accuracies = []\n",
    "    def _sigmoid(self, x):\n",
    "        return np.array([self._sigmoid_function(value) for value in x])\n",
    "\n",
    "    def _sigmoid_function(self, x):\n",
    "        if x >= 0:\n",
    "            z = np.exp(-x)\n",
    "            return 1 / (1 + z)\n",
    "        else:\n",
    "            z = np.exp(x)\n",
    "            return z / (1 + z)\n",
    "\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        # binary cross entropy\n",
    "        y_zero_loss = y_true * np.log(y_pred + 1e-9)\n",
    "        y_one_loss = (1-y_true) * np.log(1 - y_pred + 1e-9)\n",
    "        return -np.mean(y_zero_loss + y_one_loss)\n",
    "\n",
    "    def compute_gradients(self, x, y_true, y_pred):\n",
    "        # derivative of binary cross entropy\n",
    "        difference =  y_pred - y_true\n",
    "        gradient_b = np.mean(difference)\n",
    "        gradients_w = np.matmul(x.transpose(), difference)\n",
    "        gradients_w = np.array([np.mean(grad) for grad in gradients_w])\n",
    "\n",
    "        return gradients_w, gradient_b\n",
    "\n",
    "    def update_model_parameters(self, error_w, error_b):\n",
    "        self.weights = self.weights - 0.1 * error_w\n",
    "        self.bias = self.bias - 0.1 * error_b\n",
    "    \n",
    "    def _transform_x(self, x):\n",
    "        x = copy.deepcopy(x)\n",
    "        return x\n",
    "\n",
    "    def _transform_y(self, y):\n",
    "        y = copy.deepcopy(y)\n",
    "        return y.reshape(y.shape[0], 1)\n",
    "\n",
    "    def fit(self, x, y, epochs):\n",
    "        x = self._transform_x(x)\n",
    "        y = self._transform_y(y)\n",
    "\n",
    "        self.weights = np.zeros(x.shape[1])\n",
    "        self.bias = 0\n",
    "\n",
    "        for i in range(epochs):\n",
    "            x_dot_weights = np.matmul(self.weights, x.transpose()) + self.bias\n",
    "            pred = self._sigmoid(x_dot_weights)\n",
    "            loss = self.compute_loss(y, pred)\n",
    "            error_w, error_b = self.compute_gradients(x, y, pred)\n",
    "            self.update_model_parameters(error_w, error_b)\n",
    "\n",
    "            pred_to_class = [1 if p > 0.5 else 0 for p in pred]\n",
    "            self.train_accuracies.append(accuracy_score(y, pred_to_class))\n",
    "            self.losses.append(loss)\n",
    "            print(f\"Epoch: {i}, Loss: {loss}, Train Accuracy: {self.train_accuracies[-1]}\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        x_dot_weights = np.matmul(x, self.weights.transpose()) + self.bias\n",
    "        probabilities = self._sigmoid(x_dot_weights)\n",
    "        return [1 if p > 0.5 else 0 for p in probabilities]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://developer.ibm.com/articles/implementing-logistic-regression-from-scratch-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.413564</td>\n",
       "      <td>0.730614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.770186</td>\n",
       "      <td>1.430121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels        X1        X2\n",
       "0       0 -0.413564  0.730614\n",
       "1       0 -0.770186  1.430121"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "def sklearn_to_df(X_data, feature_names, label):\n",
    "    \n",
    "    x = pd.DataFrame(X_data, columns=feature_names)\n",
    "\n",
    "    y_data = label\n",
    "    y = pd.Series(y_data, name=\"label\")\n",
    "\n",
    "    return x, y\n",
    "\n",
    "x, y = sklearn_to_df(df[[\"X1\", \"X2\"]], [\"X1\", \"X2\"], df[\"labels\"])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.163413</td>\n",
       "      <td>-0.364882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>4.126459</td>\n",
       "      <td>3.610493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1        X2\n",
       "29  -1.163413 -0.364882\n",
       "535  4.126459  3.610493"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6931471785599465, Train Accuracy: 0.535\n",
      "Epoch: 1, Loss: 10.3029516138035, Train Accuracy: 0.735\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegressionFromScratch()\n",
    "model.fit(x_train.values, y_train.values, 2)\n",
    "pred2 = model.predict(x_test)\n",
    "accuracy2 = accuracy_score(y_test, pred2)\n",
    "print(accuracy2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
